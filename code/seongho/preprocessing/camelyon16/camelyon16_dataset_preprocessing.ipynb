{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19af5ac1-58b4-4d33-9200-b3f2b7574136",
   "metadata": {},
   "source": [
    "# 1. Convert lesion annotations to masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8556a4-cf61-4dc8-8b5b-8271ff6fae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the executable file from https://github.com/computationalpathologygroup/ASAP/releases, and append the binary folder to the path.\n",
    "import sys\n",
    "sys.path.append('/opt/ASAP/bin')\n",
    "\n",
    "# Import the multiresolutionimageinterface.py file from the bin folder.\n",
    "import multiresolutionimageinterface as mir\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import colorsys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import itertools\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef366bc-d765-4f3b-9af1-87c466d260db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load tif files and xml format annotation files\n",
    "reader = mir.MultiResolutionImageReader()\n",
    "annotation_list = mir.AnnotationList()\n",
    "xml_repository = mir.XmlRepository(annotation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d2ea5-290e-4026-9fc5-ede20bcb97d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addresses for 'YOUR' annotation files and training WSI (Whole Slide Images) files\n",
    "dirAnnotations = '/home/team1/ddrive/team1/camelyon16/annotations'\n",
    "dirData = '/home/team1/ddrive/team1/camelyon16/raw_data'\n",
    "dirHome = '/home/team1/ddrive/team1/camelyon16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3785d-511a-4f56-9c86-ea6cc77aa5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store only filenames in the format '~/data/training/center_0\\\\patient_001_node_1.tif' in the list\n",
    "ImageFiles = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(dirData):\n",
    "    for file in f:\n",
    "        if '.tif' in file and 'mask' not in file:\n",
    "            ImageFiles.append(os.path.join(r, file))\n",
    "\n",
    "ImageFiles.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144a30b-1d52-42cf-8e77-d426b16cef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a folder where XML files are located and convert each annotation file into a mask (tumorous areas in white, non-tumorous areas in black)\n",
    "def CreateAnnotationMask(annotationPath):\n",
    "    \n",
    "    # Store only the name of the XML file, excluding the directory and extension, e.g., tumor_001\n",
    "    fileNamePart = annotationPath.replace('.xml','').replace(dirAnnotations, \"\")\n",
    "    \n",
    "    # Add .tif extension to fileNamePart, e.g., tumor_001.tif\n",
    "    tifName = fileNamePart + '.tif'\n",
    "\n",
    "    # If there is no matching value in ImageFiles for tifName, skip -> this is only executed for images with annotations (where tumor exists).\n",
    "    partialMatches = [s for s in ImageFiles if tifName in s]\n",
    "    if len(partialMatches) == 0:\n",
    "        print('Warning - This file is missing from the file list: {0} - skipping.'.format(tifName))\n",
    "        return\n",
    "    tifPath = partialMatches[0]\n",
    "    \n",
    "    # If the tif file does not exist, skip\n",
    "    if (not os.path.isfile(tifPath)): \n",
    "        print('Warning - Could not locate {0} - skipping this annotation file.'.format(tifPath))\n",
    "        return\n",
    "    \n",
    "    # If a file already exists, skip\n",
    "    maskPath = tifPath.replace('.tif', '_mask.tif')\n",
    "    if (os.path.isfile(maskPath)):\n",
    "        print('Info - Mask file of {0} already exists - skipping'.format(tifPath))\n",
    "        return\n",
    "    \n",
    "    # Fetch the XML file\n",
    "    xml_repository.setSource(annotationPath)\n",
    "    xml_repository.load()\n",
    "\n",
    "    # Convert the XML file with polygons into a mask tif file\n",
    "    annotation_mask = mir.AnnotationToMask()\n",
    "    mr_image = reader.open(tifPath)\n",
    "    if(mr_image is None):\n",
    "        print('Warning - Could not read {0} - skipping'.format(tifPath))\n",
    "        return\n",
    "    label_map = {'metastases': 1, 'normal': 2}\n",
    "    conversion_order = ['metastases', 'normal']\n",
    "    annotation_mask.convert(annotation_list, \n",
    "                            maskPath, \n",
    "                            mr_image.getDimensions(), \n",
    "                            mr_image.getSpacing(), \n",
    "                            label_map, \n",
    "                            conversion_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b84d36-784f-4697-be58-aabe582bc89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the names of annotation files\n",
    "AnnotationFiles = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(dirAnnotations):\n",
    "    for file in f:\n",
    "        if '.xml' in file:\n",
    "            AnnotationFiles.append(os.path.join(r, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa0a19-1c61-435a-be60-24d48c88a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnnotationFiles.sort()\n",
    "AnnotationFiles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d12cf-e031-48ab-9938-e52bc4605d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for f in tqdm(AnnotationFiles, 'Creating masks...'):\n",
    "    print('Annotation file: ' + f)\n",
    "    CreateAnnotationMask(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734916da-9ab8-4b94-915c-b78f87adf59b",
   "metadata": {},
   "source": [
    "# 2. making tissue masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92386992-92fd-4a76-baaf-44f98f4685ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is adapted from a digital pathology pipeline code of Mikko Tukiainen\n",
    "# Functions identical to those in the wsi2tissueMask.py file in the utils folder.\n",
    "# Background is set to 0, and tissue parts to 255\n",
    "def make_tissue_mask(slide, mask_level=4, morpho=None, morpho_kernel_size=5, morpho_iter=1, median_filter=False, return_original=False):\n",
    "    ''' make tissue mask\n",
    "        return tissue mask array which has tissue locations (pixel value 0 -> empty, 255 -> tissue)\n",
    "    Args:\n",
    "        slide (MultiResolutionImage): MultiResolutionImage slide to process\n",
    "        mask_level (int): defines the level of zoom at which the mask will be created (default 4)\n",
    "        morpho (cv2.MORPHO): OpenCV morpho flag, Cv2.MORPHO_OPEN or Cv2.MORPHO_CLOSE (default None)\n",
    "        morpho_kernel_size (int): kernel size for morphological transformation (default 5)\n",
    "        morpho_iter (int): morphological transformation iterations (default=1)\n",
    "        median_filter (bool): Use median filtering to remove noise (default False)\n",
    "        return_original (bool): return also the unmasked image\n",
    "    '''\n",
    "    \n",
    "    # Read the slide\n",
    "    ds = slide.getLevelDownsample(mask_level)\n",
    "    original_tissue = slide.getUCharPatch(0,\n",
    "                                          0,\n",
    "                                          int(slide.getDimensions()[0] / float(ds)),\n",
    "                                          int(slide.getDimensions()[1] / float(ds)),\n",
    "                                          mask_level)\n",
    "    \n",
    "    # Extract only the brightness channel of the mask and binarize according to the threshold\n",
    "    tissue_mask = cv2.cvtColor(np.array(original_tissue), cv2.COLOR_RGBA2RGB)\n",
    "    tissue_mask = cv2.cvtColor(tissue_mask, cv2.COLOR_BGR2HSV)\n",
    "    tissue_mask = tissue_mask[:, :, 1]\n",
    "    _, tissue_mask = cv2.threshold(tissue_mask, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Apply morphological transformations\n",
    "    if morpho is not None:\n",
    "        kernel = np.ones((morpho_kernel_size, morpho_kernel_size), np.uint8)\n",
    "        tissue_mask = cv2.morphologyEx(tissue_mask, morpho, kernel, iterations=morpho_iter)\n",
    "    \n",
    "    # Remove noise with median filtering\n",
    "    if median_filter:\n",
    "        tissue_mask = cv2.medianBlur(tissue_mask, 15)\n",
    "    \n",
    "    # Convert mask to numpy array\n",
    "    tissue_mask = np.array(tissue_mask, dtype=np.uint8)\n",
    "\n",
    "    # Decide whether to also return the original\n",
    "    if return_original:\n",
    "        return tissue_mask, original_tissue\n",
    "    else:\n",
    "        return tissue_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f849376-b9ac-41f1-9599-a980257cc651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTissueMask(tifPath):\n",
    "    \n",
    "    # Extract only the filename\n",
    "    fileNamePart = tifPath.replace('.tif','').replace(dirData, \"\")\n",
    "    \n",
    "    # Skip if this mask is already found\n",
    "    maskPath = tifPath.replace('.tif', '_tissue_mask_ds16.npy')\n",
    "    if (os.path.isfile(maskPath)):\n",
    "        print('Info - Tissue mask file of {0} already exists - skipping'.format(tifPath))\n",
    "        return\n",
    "    \n",
    "    # Create tissue mask\n",
    "    mr_image = reader.open(tifPath)\n",
    "    if(mr_image is None):\n",
    "        print('Warning - Could not read {0} - skipping'.format(tifPath))\n",
    "        return\n",
    "    tissue_mask = make_tissue_mask(mr_image,\n",
    "                                   # mr_image.getBestLevelForDownSample(16), \n",
    "                                   1,\n",
    "                                   morpho=cv2.MORPH_CLOSE,\n",
    "                                   morpho_kernel_size=7,\n",
    "                                   morpho_iter=2,\n",
    "                                   median_filter=True)\n",
    "    # tissue_mask is a binary array dtype.uint8 (16 times downsampled)\n",
    "    np.save(maskPath, tissue_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3165a9f7-e0cd-4f78-81db-a77574e44a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm_notebook(ImageFiles, 'Creating tissue masks...'):\n",
    "    print('WSI: ' + f)\n",
    "    CreateTissueMask(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f6ba7e-beea-4dbd-9fd7-53e8d6cbb536",
   "metadata": {},
   "source": [
    "# 3. Create a DataFrames to record patch information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6cbb9-8e79-4ce7-9b84-3fe033442205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTissueMask(tifPath):\n",
    "    maskPath = tifPath.replace('.tif', '_tissue_mask_ds16.npy')\n",
    "    if (not os.path.isfile(maskPath)): return None\n",
    "    return np.load(maskPath)\n",
    "\n",
    "# Function to create the center positions of samples (patches)\n",
    "def sample_centers(tissue_mask, mask_downscale=16, sample_side=512, focus_width_percentage=0.25, padding_percentage=0.01):\n",
    "    # Width and height of the tissue mask\n",
    "    mask_width, mask_height = tissue_mask.shape[:2]\n",
    "\n",
    "    # Sample size\n",
    "    side = sample_side / mask_downscale\n",
    "\n",
    "    # Padding size\n",
    "    padding_width = mask_width * padding_percentage\n",
    "    padding_height = mask_height * padding_percentage\n",
    "\n",
    "    # Half-width of the focus area\n",
    "    half_focus = int(sample_side * focus_width_percentage / mask_downscale)\n",
    "    \n",
    "    # List to store the center coordinates of the samples\n",
    "    sample_centers = []\n",
    "    \n",
    "    # Determine sample centers based on areas where tissue exists\n",
    "    for i in range(int(mask_width // side)):\n",
    "        for j in range(int(mask_height // side)):\n",
    "            for sub_shift in [0, 0.5]:\n",
    "                x = int((i + sub_shift) * side)\n",
    "                y = int((j + sub_shift) * side)\n",
    "                min_x = int(max(0, x - half_focus))\n",
    "                max_x = int(min(x + half_focus, mask_width - 1))\n",
    "                min_y = int(max(0, y - half_focus))\n",
    "                max_y = int(min(y + half_focus, mask_height - 1))\n",
    "                \n",
    "                # Skip samples in the padding area\n",
    "                if(min_x < padding_width or max_x > mask_width - padding_width): continue\n",
    "                if(min_y < padding_height or max_y > mask_height - padding_height): continue\n",
    "                \n",
    "                # Add to samples only areas where tissue exists\n",
    "                if(tissue_mask[min_x:max_x, min_y:max_y].sum() > 0):\n",
    "                    sample_centers.append(np.array([x, y]))\n",
    "                    \n",
    "    # Restore the mask downscale to compute coordinates\n",
    "    sample_centers = np.array(sample_centers) * mask_downscale\n",
    "    return sample_centers\n",
    "\n",
    "# Check if there is a tumor in the patch\n",
    "def isTumor(mask_level_0):\n",
    "    return (mask_level_0.max() > 0)\n",
    "\n",
    "# Calculate the percentage of tumor in the patch\n",
    "def tumorPercentage(mask_level_0):\n",
    "    area = mask_level_0.shape[0] * mask_level_0.shape[1]\n",
    "    tumorPixels = np.count_nonzero(mask_level_0)\n",
    "    channels = 3\n",
    "    return tumorPixels / (area * channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b04f6c-d1a4-4117-abca-139be968b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = mir.MultiResolutionImageReader()\n",
    "\n",
    "# Load image\n",
    "def getImage(tifPath):\n",
    "    if (not os.path.isfile(tifPath)): return None\n",
    "    return reader.open(tifPath)\n",
    "\n",
    "# Load mask file (only the tumorous parts)\n",
    "def getAnnoMask(tifPath):\n",
    "    maskPath = tifPath.replace('.tif', '_mask.tif')\n",
    "    if (not os.path.isfile(maskPath)): return None\n",
    "    return reader.open(maskPath)\n",
    "\n",
    "\n",
    "def getSamplesWithAnnotations(mr_image, mr_mask, x_cent, y_cent, width=512, height=512):\n",
    "    channels = 3\n",
    "    imgs = np.zeros((1, width, height, channels), dtype=np.int32)\n",
    "    masks = np.zeros((1, width, height, channels), dtype=np.int32)\n",
    "\n",
    "    lev = mr_image.getBestLevelForDownSample(1)\n",
    "    ds = mr_image.getLevelDownsample(lev)\n",
    "    imgs[0] = mr_image.getUCharPatch(int(x_cent - (ds*width/2)),\n",
    "                                     int(y_cent - (ds*height/2)),\n",
    "                                     width,\n",
    "                                     height,\n",
    "                                     lev)\n",
    "    masks[0] = mr_mask.getUCharPatch(int(x_cent - (ds*width/2)),\n",
    "                                     int(y_cent - (ds*height/2)),\n",
    "                                     width,\n",
    "                                     height,\n",
    "                                     lev)\n",
    "    return imgs, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695e7a7-4a21-472f-8afe-823107435919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a WSI file into patches and create a CSV file storing annotations for each patch\n",
    "def CreateDF(tifPath, overrideExisting=False, size=256):\n",
    "    # How many times to multiply the 16x reduced tissue mask. Since it's 16, it's equivalent to 400x original magnification\n",
    "    mask_downscales = [16]\n",
    "    mags = ['400x']\n",
    "    \n",
    "    mr_image = getImage(tifPath)\n",
    "    mr_mask = getAnnoMask(tifPath)\n",
    "\n",
    "    for i, (mask_downscale, mag) in enumerate(zip(mask_downscales, mags)):\n",
    "        \n",
    "        # Only store the file name\n",
    "        fileNamePart = tifPath.replace('.tif','').replace(dirData, \"\")\n",
    "        df_path = dirHome + '/dataframes/' + fileNamePart.split('/')[-1] + '.csv'\n",
    "    \n",
    "        if (os.path.isfile(df_path) and not overrideExisting):\n",
    "            print('Info - Dataframe file of {0} already exists - skipping'.format(df_path))\n",
    "            continue\n",
    "        \n",
    "        tissue_mask = getTissueMask(tifPath)\n",
    "        patch_centers = sample_centers(tissue_mask, mask_downscale=mask_downscale, sample_side=size)\n",
    "\n",
    "    \n",
    "        print(\"Sliced WSI {1} to {0} patches.\".format(len(patch_centers), tifPath))\n",
    "        \n",
    "        # Load the current image file/mask file\n",
    "\n",
    "        \n",
    "        df = pd.DataFrame(columns=['patchId',\n",
    "                                   'fileName',\n",
    "                                   'centerX',\n",
    "                                   'centerY',\n",
    "                                   'isTumor',\n",
    "                                   'tumorPercentage'\n",
    "                                    ])\n",
    "        \n",
    "        # If the directory is different, it needs to be changed.\n",
    "        wsi_name = tifPath.split('/')[-1]\n",
    "        tumor_idx = wsi_name.strip('.tif').split('_')[-1]\n",
    "        \n",
    "        for c in tqdm_notebook(patch_centers, 'Patches...'):\n",
    "            imgs, masks = getSamplesWithAnnotations(mr_image, mr_mask, x_cent=c[1], y_cent=c[0], width=size, height=size)\n",
    "\n",
    "            isTumor_attr = isTumor(masks[i])\n",
    "            tumorPrc_attr = tumorPercentage(masks[i])\n",
    "            \n",
    "            df = df.append({'patchId': str(tumor_idx) + '_' + str(c[0]).zfill(7) + str(c[1]).zfill(7),\n",
    "                           'fileName': tifPath,\n",
    "                           'centerX': c[0],\n",
    "                           'centerY': c[1],\n",
    "                           'isTumor': isTumor_attr,\n",
    "                           'tumorPercentage': int(tumorPrc_attr * 1000) / 10}, ignore_index=True)\n",
    "    \n",
    "        df.to_csv(df_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68cedb7-adfe-415f-a1e4-15cb48a3b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm_notebook(ImageFiles, 'Creating dataframes...'):\n",
    "    try:\n",
    "        CreateDF(f)\n",
    "    except: \n",
    "        print('Cannot find the file..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184c3c48-cb73-44c4-95e6-779ec8a3894c",
   "metadata": {},
   "source": [
    "## Merging dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00902eee-7033-468a-901a-757d1d8ba4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory path\n",
    "directory = dirHome + '/dataframes'\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# Browse all files in the directory and add the DataFrames to the list\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a826a615-77c1-4237-8925-41c5b48fae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "merged_df = merged_df.drop('Unnamed: 0', axis=1)\n",
    "merged_df['wsi_id'] = merged_df['patchId'].apply(lambda x: x.split('_')[0])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a80279-099b-47e3-b120-1da3e3622ae9",
   "metadata": {},
   "source": [
    "# 4. Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d1e26-9bab-4dfd-95ec-55395e443869",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df = positive_df[(positive_df['tumorPercentage'] > 20) & (positive_df['tumorPercentage'] < 90)]\n",
    "positive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce395a5-e384-480b-afd3-d1fa8a055e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comb = ['061', '018', '039', '031', '035', '051', '064', '040', '030', '007', '025', '041',\n",
    " '054', '065', '005', '046', '023', '004', '022', '008', '006', '068', '001', '009',\n",
    " '015', '042', '016', '056', '027', '057', '038', '063', '019', '045', '037', '062',\n",
    " '058', '033', '010', '002', '060', '055', '069', '052', '066', '011', '024']\n",
    "\n",
    "val_comb = ['048', '013', '067', '044', '043', '036']\n",
    "\n",
    "test_comb = ['032', '047', '049', '028', '050', '053', '034' ,'012' ,'059', '014', '020' ,'003',\n",
    " '026','029' ,'017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e41088-d3e1-4240-a2b4-0a716ce82d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataframe\n",
    "df_train = positive_df[positive_df['wsi_id'].isin(train_comb)]\n",
    "df_valid = positive_df[positive_df['wsi_id'].isin(val_comb)]\n",
    "df_test = positive_df[positive_df['wsi_id'].isin(test_comb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817b077-2a6a-46ae-9dd9-032c96409d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train['isTumor'].value_counts())\n",
    "print(df_valid['isTumor'].value_counts())\n",
    "print(df_test['isTumor'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca7cb4-1578-4a89-b6cd-a37dd5f3b0cd",
   "metadata": {},
   "source": [
    "## Training - Positive Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8551ee4-f44f-43e5-85db-cba070d7a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame\n",
    "df_train_copy = df_train.copy()\n",
    "\n",
    "# Define intervals (0, 5, 10, ..., 100)\n",
    "bins = np.arange(0, 105, 5)\n",
    "\n",
    "# Divide 'tumorPercentage' values into 5-unit intervals\n",
    "df_train_copy['range'] = pd.cut(df_train_copy['tumorPercentage'], bins, right=False)\n",
    "\n",
    "# Sample up to 430 items per interval\n",
    "def sample_per_group(x):\n",
    "    n_samples = 430\n",
    "    return x.sample(n=min(len(x), n_samples), random_state=42) if len(x) > 0 else x\n",
    "\n",
    "# Add include_groups=False to groupby to resolve a warning\n",
    "df_train_sampled = df_train_copy.groupby('range', as_index=False, observed=True).apply(sample_per_group).reset_index(drop=True)\n",
    "\n",
    "df_train_sampled.drop('range', axis=1, inplace=True)\n",
    "\n",
    "df_train_sampled['tumorPercentage'].plot(kind='hist', bins=20)  # Adjust the bins value from 19 to 20\n",
    "df_train_sampled.to_csv(dirHome + '/sample_patches_train_6020.csv', index=False)\n",
    "print(len(df_train_sampled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820fce3f-9a7b-447f-8a78-e31fd829e7d4",
   "metadata": {},
   "source": [
    "## Train - Negative Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e790bd-a0e2-491b-8498-be630764fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_neg_all = merged_df[(merged_df['wsi_id'].isin(train_comb)) & (merged_df['isTumor'] == False)]\n",
    "df_train_neg = df_train_neg_all.sample(n=6000, random_state=42)\n",
    "print(df_train_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0a261-2325-40a3-8b35-7cb4060321ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_neg.to_csv(dirHome + '/sample_patches_negative_train_6020.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6ed5c-8fef-4029-8b7a-7661ae0e9feb",
   "metadata": {},
   "source": [
    "## Valid - Positive Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646430c8-db71-4b54-b6df-fd1ebaf9ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame\n",
    "df_valid_copy = df_valid.copy()\n",
    "\n",
    "# Define intervals (0, 5, 10, ..., 100)\n",
    "bins = np.arange(0, 105, 5)\n",
    "\n",
    "# Divide 'tumorPercentage' values into 5-unit intervals\n",
    "df_valid_copy['range'] = pd.cut(df_valid_copy['tumorPercentage'], bins, right=False)\n",
    "\n",
    "# Sample up to 50 items per interval\n",
    "def sample_per_group(x):\n",
    "    n_samples = 50\n",
    "    return x.sample(n=min(len(x), n_samples), random_state=42) if len(x) > 0 else x\n",
    "\n",
    "# Add include_groups=False to groupby to resolve a warning\n",
    "df_valid_sampled = df_valid_copy.groupby('range', as_index=False, observed=True).apply(sample_per_group).reset_index(drop=True)\n",
    "df_valid_sampled.drop('range', axis=1, inplace=True)\n",
    "\n",
    "df_valid_sampled['tumorPercentage'].plot(kind='hist', bins=20)  # Adjust the bins value from 19 to 20\n",
    "df_valid_sampled.to_csv(dirHome + '/sample_patches_valid_6020.csv', index=False)\n",
    "print(len(df_valid_sampled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cdf20-4b96-4276-b7b5-0cff83ec4f6c",
   "metadata": {},
   "source": [
    "## Valid - Negative Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9b057-c62e-4e20-a4d9-27844adf5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_neg_all = merged_df[(merged_df['wsi_id'].isin(val_comb)) & (merged_df['isTumor'] == False)]\n",
    "df_valid_neg = df_valid_neg_all.sample(n=700, random_state=42)\n",
    "print(df_valid_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9deac7-e45f-4cd8-9377-6aab3e6a29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_neg.to_csv(dirHome + '/sample_patches_negative_valid_6020.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0517e33-49bf-44d3-a20a-14b128d0a272",
   "metadata": {},
   "source": [
    "## Test - Positive Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd37b6-79ff-4913-95c3-ca002f91239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame\n",
    "df_test_copy = df_test.copy()\n",
    "\n",
    "# Define intervals (0, 5, 10, ..., 100)\n",
    "bins = np.arange(0, 105, 5)\n",
    "\n",
    "# Divide 'tumorPercentage' values into 5-unit intervals\n",
    "df_test_copy['range'] = pd.cut(df_test_copy['tumorPercentage'], bins, right=False)\n",
    "\n",
    "# Sample up to 143 items per interval\n",
    "def sample_per_group(x):\n",
    "    n_samples = 143\n",
    "    return x.sample(n=min(len(x), n_samples), random_state=42) if len(x) > 0 else x\n",
    "\n",
    "# Add include_groups=False to groupby to resolve a warning\n",
    "df_test_sampled = df_test_copy.groupby('range', as_index=False, observed=True).apply(sample_per_group).reset_index(drop=True)\n",
    "\n",
    "print(len(df_test_sampled))\n",
    "# Check the number of samples in each interval\n",
    "\n",
    "df_test_sampled.drop('range', axis=1, inplace=True)\n",
    "\n",
    "df_test_sampled['tumorPercentage'].plot(kind='hist', bins=20)  # Adjust the bins value from 19 to 20\n",
    "df_test_sampled.to_csv(dirHome + '/sample_patches_test_6020.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c96ef-5de0-4986-8546-6feebe8d7e2a",
   "metadata": {},
   "source": [
    "## Test - Negative Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f7fa69-d775-449a-81e8-f34d38283877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_neg_all = merged_df[(merged_df['wsi_id'].isin(test_comb)) & (merged_df['isTumor'] == False)]\n",
    "df_test_neg = df_test_neg_all.sample(n=2000, random_state=42)\n",
    "print(df_test_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac12349-cec0-46c5-bd81-1469e716c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_neg.to_csv(dirHome + '/sample_patches_negative_test_6020.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61a20a-2685-43a9-9c31-618845e5f173",
   "metadata": {},
   "source": [
    "# 5. Creating Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab9151a-af0d-4c2b-97a5-5f7ea36f086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSamples(mr_image, x_cent, y_cent, levels, sz):\n",
    "    channels = 3\n",
    "    imgs = np.zeros((len(levels), sz, sz, channels), dtype=np.uint8)\n",
    "    for i, lev in enumerate(levels):\n",
    "        ds = mr_image.getLevelDownsample(lev)\n",
    "        imgs[i] = mr_image.getUCharPatch(int(x_cent - (ds*sz/2)),\n",
    "                                         int(y_cent - (ds*sz/2)),\n",
    "                                         sz,\n",
    "                                         sz,\n",
    "                                         lev)\n",
    "    return imgs\n",
    "\n",
    "def getMaskedSamples(mr_mask, x_cent, y_cent, levels, sz):\n",
    "    masks = np.zeros((len(levels), sz, sz), dtype=np.uint8)\n",
    "    for i, lev in enumerate(levels):\n",
    "        ds = mr_mask.getLevelDownsample(lev)\n",
    "        mask = mr_mask.getUCharPatch(int(x_cent - (ds*sz/2)),\n",
    "                                     int(y_cent - (ds*sz/2)),\n",
    "                                     sz,\n",
    "                                     sz,\n",
    "                                     lev)\n",
    "        # Select and use only the red channel.\n",
    "        masks[i] = mask[:, :, 0]  # Using only the red channel\n",
    "    return masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a6eb0-e26a-4c68-8e05-b7ae371b7890",
   "metadata": {},
   "source": [
    "## Extract Positive Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ca550-0ecf-4d5f-9364-37b1cfeb35ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(dirHome + '/sample_patches_train_6020.csv')\n",
    "df_valid = pd.read_csv(dirHome + '/sample_patches_valid_6020.csv')\n",
    "df_test = pd.read_csv(dirHome + '/sample_patches_test_6020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a713155-c326-420d-ba95-1c530ea4b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['wsi_id'] = df_train['patchId'].apply(lambda x: x.split('_')[0])\n",
    "df_valid['wsi_id'] = df_valid['patchId'].apply(lambda x: x.split('_')[0])\n",
    "df_test['wsi_id'] = df_test['patchId'].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab384863-d7b9-4bf6-8469-6f2ac7efe040",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89751d08-81af-45cc-a0f6-5bac61b5b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the directory where you want to finally save the dataset\n",
    "dirRoot = '/home/team1/ddrive/team1/dataset_dest/camelyon16'\n",
    "os.makedirs(dirRoot, exist_ok=True)\n",
    "df_list = [df_train, df_valid, df_test]\n",
    "dirName_list = ['train', 'val', 'test']\n",
    "\n",
    "levels = [0]\n",
    "\n",
    "reader = mir.MultiResolutionImageReader()\n",
    "\n",
    "for WSI in tqdm(ImageFiles, desc='Processing WSIs'):\n",
    "    mr_image = reader.open(WSI)\n",
    "    mask_image_path = WSI.replace('.tif', '_mask.tif')\n",
    "    mask_image = reader.open(mask_image_path)\n",
    "\n",
    "    split = WSI.split('/')\n",
    "    wsi_id = split[-1].split('_')[1].split('.')[0]\n",
    "    \n",
    "    for df, dirName in zip(df_list, dirName_list):\n",
    "        df_sub = df[df.wsi_id == wsi_id]\n",
    "        for i in range(len(df_sub)):\n",
    "            id = str(df_sub.iloc[i].patchId)\n",
    "            label = 1 if df_sub.iloc[i].isTumor else 0\n",
    "\n",
    "            image_dir = os.path.join(dirRoot, dirName, 'image')\n",
    "            mask_dir = os.path.join(dirRoot, dirName, 'mask')\n",
    "            os.makedirs(image_dir, exist_ok=True)  # Ensure the image directory exists\n",
    "            os.makedirs(mask_dir, exist_ok=True)   # Ensure the mask directory exists\n",
    "            \n",
    "            fileNamePrefix = os.path.join(image_dir, id)\n",
    "            center_x = df_sub.iloc[i].centerX\n",
    "            center_y = df_sub.iloc[i].centerY\n",
    "    \n",
    "            imgs = getSamples(mr_image, center_y, center_x, levels, 512)\n",
    "    \n",
    "            image_file_name = f\"{fileNamePrefix}_{label}.png\"\n",
    "            if not os.path.exists(image_file_name):\n",
    "                cv2.imwrite(image_file_name, imgs[0])\n",
    "            else:\n",
    "                print(f\"File {image_file_name} already exists, skipping.\")\n",
    "    \n",
    "            if label:\n",
    "                masks = getMaskedSamples(mask_image, center_y, center_x, levels, 512)\n",
    "                maskFileNamePrefix = os.path.join(mask_dir, id)\n",
    "                mask_file_name = f\"{maskFileNamePrefix}_{label}.png\"\n",
    "                if not os.path.exists(mask_file_name):\n",
    "                    cv2.imwrite(mask_file_name, masks[0])\n",
    "                else:\n",
    "                    print(f\"File {mask_file_name} already exists, skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd224a29-c9b0-4e7a-b987-d1d492e33902",
   "metadata": {},
   "source": [
    "## Extract Negative Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ad75ed-8836-4716-9baa-50edef910faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_neg = pd.read_csv(dirHome + '/sample_patches_negative_train_6020.csv')\n",
    "df_valid_neg = pd.read_csv(dirHome + '/sample_patches_negative_valid_6020.csv')\n",
    "df_test_neg = pd.read_csv(dirHome + '/sample_patches_negative_test_6020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dabaa25-6500-49cc-b97e-cfd42fb30eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_neg['wsi_id'] = df_train_neg['patchId'].apply(lambda x: x.split('_')[0])\n",
    "df_valid_neg['wsi_id'] = df_valid_neg['patchId'].apply(lambda x: x.split('_')[0])\n",
    "df_test_neg['wsi_id'] = df_test_neg['patchId'].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129dfc17-8a63-449a-bc87-20c19c51a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the directory where you want to finally save the dataset\n",
    "dirRoot = '/home/team1/ddrive/team1/dataset_dest/camelyon16'\n",
    "os.makedirs(dirRoot, exist_ok=True)\n",
    "df_list = [df_train_neg, df_valid_neg, df_test_neg]\n",
    "dirName_list = ['train', 'val', 'test']\n",
    "\n",
    "levels = [0]\n",
    "\n",
    "reader = mir.MultiResolutionImageReader()\n",
    "\n",
    "for WSI in tqdm(ImageFiles, desc='Processing WSIs'):\n",
    "    mr_image = reader.open(WSI)\n",
    "    mask_image_path = WSI.replace('.tif', '_mask.tif')\n",
    "    mask_image = reader.open(mask_image_path)\n",
    "\n",
    "    split = WSI.split('/')\n",
    "    wsi_id = split[-1].split('_')[1].split('.')[0]\n",
    "    \n",
    "    for df, dirName in zip(df_list, dirName_list):\n",
    "        df_sub = df[df.wsi_id == wsi_id]\n",
    "        for i in range(len(df_sub)):\n",
    "            id = str(df_sub.iloc[i].patchId)\n",
    "            label = 1 if df_sub.iloc[i].isTumor else 0\n",
    "\n",
    "            image_dir = os.path.join(dirRoot, dirName, 'image')\n",
    "            mask_dir = os.path.join(dirRoot, dirName, 'mask')\n",
    "            os.makedirs(image_dir, exist_ok=True)  # Ensure the image directory exists\n",
    "            os.makedirs(mask_dir, exist_ok=True)   # Ensure the mask directory exists\n",
    "            \n",
    "            fileNamePrefix = os.path.join(image_dir, id)\n",
    "            center_x = df_sub.iloc[i].centerX\n",
    "            center_y = df_sub.iloc[i].centerY\n",
    "    \n",
    "            imgs = getSamples(mr_image, center_y, center_x, levels, 512)\n",
    "    \n",
    "            image_file_name = f\"{fileNamePrefix}_{label}.png\"\n",
    "            if not os.path.exists(image_file_name):\n",
    "                cv2.imwrite(image_file_name, imgs[0])\n",
    "            else:\n",
    "                print(f\"File {image_file_name} already exists, skipping.\")\n",
    "    \n",
    "            if label:\n",
    "                masks = getMaskedSamples(mask_image, center_y, center_x, levels, 512)\n",
    "                maskFileNamePrefix = os.path.join(mask_dir, id)\n",
    "                mask_file_name = f\"{maskFileNamePrefix}_{label}.png\"\n",
    "                if not os.path.exists(mask_file_name):\n",
    "                    cv2.imwrite(mask_file_name, masks[0])\n",
    "                else:\n",
    "                    print(f\"File {mask_file_name} already exists, skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e5314-75d5-49d8-a433-17deac2fec7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
